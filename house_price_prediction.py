# -*- coding: utf-8 -*-
"""House_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fXQBUbOhXCrxIM2XdWm_woHcuHl29MI2

Analysis :

2) Housing Price Prediction with a neural Network.

# ***House Price Prediction***
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from pandas.api.types import CategoricalDtype
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder


filepath = '/content/sample_data/NY-House-Dataset 2.csv'
data = pd.read_csv(filepath)
data.head()

"""Price distribution"""

from matplotlib import pyplot as plt
data['PRICE'].plot(kind='hist', bins=20, title='PRICE')
plt.gca().spines[['top', 'right',]].set_visible(False)

data.describe()

data.info()

"""Data Cleaning and Feature Engineering"""

#Elimino le colonne senza valori numerici

df=data
drop_columns = ["BROKERTITLE", "TYPE", "SUBLOCALITY","ADDRESS", "STATE", "MAIN_ADDRESS", "ADMINISTRATIVE_AREA_LEVEL_2",
                "LOCALITY", "STREET_NAME", "LONG_NAME", "FORMATTED_ADDRESS", "Unnamed: 0"]
df = df.drop(columns=drop_columns, errors='ignore')

# filtro gli outlier con il metodo dei quartili

vars_to_filter = ["PRICE", "BEDS", "BATH", "PROPERTYSQFT"]
for var in vars_to_filter:
    Q1 = df[var].quantile(0.25)
    Q3 = df[var].quantile(0.75)
    IQR = Q3 - Q1
    lower_limit = Q1 - 1.5 * IQR
    upper_limit = Q3 + 1.5 * IQR
    df = df[(df[var] >= lower_limit) & (df[var] <= upper_limit)]


#Feature Engineering

from geopy.distance import geodesic

def distance_from_center(row):
    house_coords = (row["LATITUDE"], row["LONGITUDE"])
    center_coords = (40.7580, -73.9855)
    return geodesic(house_coords, center_coords).km

df["DISTANCE_FROM_CENTER"] = df.apply(distance_from_center, axis=1)  #Fondamentale

data_filtered=df
data_filtered.info()

data_filtered.describe()

"""New Price distribution"""

from matplotlib import pyplot as plt
data_filtered['PRICE'].plot(kind='hist', bins=20, title='PRICE')
plt.gca().spines[['top', 'right',]].set_visible(False)

"""Price vs distance from the center"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.scatterplot(data=data_filtered, x="DISTANCE_FROM_CENTER", y="PRICE", alpha=0.5)
plt.title("Prezzo delle case vs Distanza dal centro")
plt.xlabel("Distanza dal centro (km)")
plt.ylabel("Prezzo ($)")
plt.grid(True)
plt.show()

"""Correlation Matrix

"""

import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = data_filtered.corr(numeric_only=True)
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Matrice di correlazione tra le variabili")
plt.show()

"""Analysis"""

# train, validation ,test

from sklearn import model_selection
train, test = model_selection.train_test_split(data_filtered, test_size=0.2, random_state=42)
train, valid = model_selection.train_test_split(train, test_size=0.3, random_state=42)
train.shape, valid.shape, test.shape

x_train, y_train = train.to_numpy()[:, :-1], train.to_numpy()[:, -1]
x_valid, y_valid = valid.to_numpy()[:, :-1], valid.to_numpy()[:, -1]
x_test, y_test   = test.to_numpy()[:, :-1], test.to_numpy()[:, -1]

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr

# Linear Regression
linear = make_pipeline(StandardScaler(), LinearRegression())
linear.fit(x_train, y_train)
y_valid_hat = linear.predict(x_valid,)
mae=mean_absolute_error(y_valid, y_valid_hat)
mse = mean_squared_error(y_valid, y_valid_hat)
r2 = r2_score(y_valid, y_valid_hat)
corr_coef, _ = pearsonr(y_valid, y_valid_hat)
corr_percent = corr_coef * 100

print(f"MAE: {mae:.3f}")
print(f"MSE: {mse:.3f}")
print(f"R² Score: {r2:.3f}")
print(f"Correlazione (Pearson): {corr_coef:.3f} → {corr_percent:.2f}%")

#Random Forest
forest = make_pipeline( StandardScaler(), RandomForestRegressor())
forest.fit(x_train, y_train)
y_valid_hat = forest.predict(x_valid,)
mae=mean_absolute_error(y_valid, y_valid_hat)
mse = mean_squared_error(y_valid, y_valid_hat)
r2 = r2_score(y_valid, y_valid_hat)
corr_coef, _ = pearsonr(y_valid, y_valid_hat)
corr_percent = corr_coef * 100

print(f"MAE: {mae:.3f}")
print(f"MSE: {mse:.3f}")
print(f"R² Score: {r2:.3f}")
print(f"Correlazione (Pearson): {corr_coef:.3f} → {corr_percent:.2f}%")

# MLP

input_l = tf.keras.layers.Input(shape=(x_train.shape[1],))

l = tf.keras.layers.Dense(30, activation='relu', )(input_l)
l = tf.keras.layers.BatchNormalization()(l)
l = tf.keras.layers.Dropout(0.1)(l)
l = tf.keras.layers.Dense(20, activation='relu')(l)
l = tf.keras.layers.BatchNormalization()(l)
l = tf.keras.layers.Dropout(0.1)(l)
l = tf.keras.layers.Dense(20, activation='relu')(l)
l = tf.keras.layers.BatchNormalization()(l)
l = tf.keras.layers.Dropout(0.1)(l)
l = tf.keras.layers.Dense(10, activation='relu')(l)
output_l = tf.keras.layers.Dense(1, activation='linear')(l)

mlp = tf.keras.models.Model(input_l, output_l, )
mlp.summary()

mlp.compile(
    optimizer='adam',
    loss='mae',
    metrics=['mse']
)

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
tensorboard = tf.keras.callbacks.TensorBoard(
    log_dir='logs',
    histogram_freq=0,
    write_graph=True,
    update_freq='epoch',
    profile_batch=0,
    embeddings_freq=0,
    )

# ModelCheckPoint
checkpoint = ModelCheckpoint(
    'best_model.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min'
)

# EarlyStopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

y_train = y_train.astype(float)
y_valid = y_valid.astype(float)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_valid_scaled = scaler.transform(x_valid)
x_test_scaled = scaler.transform(x_test)


mlp.fit(x_train_scaled, y_train,
        batch_size=32,
        epochs=40,
        validation_data=(x_valid_scaled, y_valid),
        callbacks=[tensorboard,checkpoint,early_stop])

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

"""Best Model"""

#Random Forest

X_trainval = np.vstack([x_train, x_valid])
y_trainval = np.concatenate([y_train, y_valid])


forest = make_pipeline( StandardScaler(), RandomForestRegressor())
forest.fit(X_trainval, y_trainval)
y_test_hat = forest.predict(x_test)
mae=mean_absolute_error(y_test, y_test_hat)
mse = mean_squared_error(y_test, y_test_hat)
r2 = r2_score(y_test, y_test_hat)
corr_coef, _ = pearsonr(y_test, y_test_hat)
corr_percent = corr_coef * 100

print(f"MAE: {mae:.3f}")
print(f"MSE: {mse:.3f}")
print(f"R² Score: {r2:.3f}")
print(f"Correlazione (Pearson): {corr_coef:.3f} → {corr_percent:.2f}%")